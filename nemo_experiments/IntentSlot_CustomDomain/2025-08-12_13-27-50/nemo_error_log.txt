[NeMo W 2025-08-12 13:27:51 save_restore_connector:430] src path does not exist or it is not a path in nemo file. src value I got was: ../models/nlp/tokenizer.vocab_file. Absolute: /mnt/c/Users/Sammy/GitHubProjects/models/nlp/tokenizer.vocab_file
[NeMo W 2025-08-12 13:27:51 modelPT:291] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.
[NeMo W 2025-08-12 13:27:51 nlp_overrides:1048] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex
    Megatron-based models require Apex to function correctly.
[NeMo W 2025-08-12 13:27:51 save_restore_connector:430] src path does not exist or it is not a path in nemo file. src value I got was: ../models/nlp/distilbert-base-uncased_encoder_config.json. Absolute: /mnt/c/Users/Sammy/GitHubProjects/models/nlp/distilbert-base-uncased_encoder_config.json
[NeMo W 2025-08-12 13:27:58 lr_scheduler:865] `max_steps` is set to -1 in the scheduler config, scheduler will not be instantiated
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('intent_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('intent_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('intent_f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('slot_precision', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('slot_recall', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2025-08-12 13:27:59 nemo_logging:361] /home/sammy/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('slot_f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
