commit hash: 36440df770a7ef2e17d5d601d665a7e38ef8f1af
diff --git a/README.md b/README.md
index ee99273..cebe87b 100644
--- a/README.md
+++ b/README.md
@@ -1,20 +1,24 @@
-# Voice Control Assistant with RIVA
-
-Project uses Nvidia RIVA with pretrained models for a full pipeline from ASR, NLP, and TTS that can query from SQLite with voice.
-
-## Purpose for this project
-
-I wanted to build an voice assistant to see if it could retrieve information from a database easily. I thought there was not many good solutions that provided a lot of the tooling that could be done out-of-the-box without doing a lot of configuration and experimentation.
-
-### Links for General RIVA
-- https://www.nvidia.com/en-us/ai-data-science/products/riva/
-- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html
-- https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart
-- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html
-- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/installation/deploy-local.html#local-docker
-- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html
-- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix/support-matrix-older-versions.html#riva-2-17-0
-- https://arizsiddiqui.medium.com/building-powerful-conversational-ai-models-with-nvidia-nemo-ce63243284d2
-- https://github.com/nvidia-riva/tutorials?tab=readme-ov-file#running-the-riva-client
-
-### Links for the NGC models used
+# Voice Control Assistant with RIVA
+
+Project uses Nvidia RIVA with pretrained models for a full pipeline from ASR, NLP, and TTS that can query from SQLite with voice.
+
+## Purpose for this project
+
+I wanted to build an voice assistant to see if it could retrieve information from a database easily. I thought there was not many good solutions that provided a lot of the tooling that could be done out-of-the-box without doing a lot of configuration and experimentation.
+
+### Links for General RIVA
+- https://www.nvidia.com/en-us/ai-data-science/products/riva/
+- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html
+- https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart
+- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html
+- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/installation/deploy-local.html#local-docker
+- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html
+- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix/support-matrix-older-versions.html#riva-2-17-0
+- https://arizsiddiqui.medium.com/building-powerful-conversational-ai-models-with-nvidia-nemo-ce63243284d2
+- https://github.com/nvidia-riva/tutorials?tab=readme-ov-file#running-the-riva-client
+-https://docs.nvidia.com/deeplearning/riva/archives/170-b/user-guide/docs/notebooks/Riva_speech_API_demo.html
+- https://github.com/nvidia-riva/python-clients
+- https://docs.nvidia.com/nemo-framework/user-guide/24.07/nemotoolkit/asr/speech_intent_slot/configs.html
+
+### Links for the NGC models used
+- https://docs.nvidia.com/tao/tao-toolkit-archive/tao-30-2202/text/nlp/intent_and_slot.html
diff --git a/main.py b/main.py
index aff37b8..40d6f4c 100644
--- a/main.py
+++ b/main.py
@@ -1,69 +1,28 @@
-import io
-import IPython.display as ipd
-import grpc
-import numpy as np
-import riva.client
-import wave
-import platform
-import os
-
-auth = riva.client.Auth(uri='localhost:50051')
-
-riva_asr = riva.client.ASRService(auth)
-riva_tts = riva.client.SpeechSynthesisService(auth)
-
-path = "audio_samples/output_test.wav"
-
-SAMPLE_RATE_HERTZ = 16000 
-LANGUAGE_CODE = "en-US"
-AUDIO_ENCODINGS = riva.client.AudioEncoding.LINEAR_PCM 
-
-def transcribe_audio_with_asr(path: str):
-    with io.open(path, 'rb') as fh:
-        content = fh.read()
-    ipd.Audio(path)
-
-    # Set up an offline/batch recognition request
-    config = riva.client.RecognitionConfig()
-    config.encoding = AUDIO_ENCODINGS # Audio encoding can be detected from wav
-    config.sample_rate_hertz = SAMPLE_RATE_HERTZ      # Sample rate can be detected from wav and resampled if needed
-    config.language_code = LANGUAGE_CODE              # Language code of the audio clip
-    config.max_alternatives = 1                       # How many top-N hypotheses to return
-    config.enable_automatic_punctuation = True        # Add punctuation when end of VAD detected
-    config.audio_channel_count = 1                    # Mono channel
-
-    response = riva_asr.offline_recognize(content, config)
-
-    asr_best_transcript = response.results[0].alternatives[0].transcript
-    print("ASR Transcript:", asr_best_transcript)
-    print("ASR Response:", response) #for debugging purposes
-    return asr_best_transcript, response
-
-def save_tts_audio_as_wav(audio_bytes, filename):
-    sample_rate = 48000
-    with wave.open(filename, 'wb') as wf:
-        wf.setnchannels(1)          # Mono channel
-        wf.setsampwidth(2)          # 16-bit audio (2 bytes per sample)
-        wf.setframerate(sample_rate) # Sample rate (Hz)
-        wf.writeframes(audio_bytes)
-    print(f"Saved synthesized speech to {filename}")
-
-
-def output_text_to_speech(asr_best_transcript: str):
-    req = {
-        "language_code": LANGUAGE_CODE,
-        "encoding": AUDIO_ENCODINGS,
-        "voice_name": "English-US.Female-1",
-        "text": asr_best_transcript
-    }
-
-    resp = riva_tts.synthesize(**req)
-    audio_samples = np.frombuffer(resp.audio, dtype=np.int16)
-    ipd.Audio(audio_samples, rate=SAMPLE_RATE_HERTZ)
-    print("TTS Response:", resp)  # for debugging purposes
-
-    save_tts_audio_as_wav(resp.audio, "tts_output.wav")
-
-asr_best_transcript, response = transcribe_audio_with_asr(path)
-
-output_text_to_speech(asr_best_transcript)
\ No newline at end of file
+from riva_client.asr_client import RivaASRClient
+from riva_client.nlp_client import RivaNLUClient
+from riva_client.tts_client import RivaTTSClient
+from db.db_helper import SQLiteHelper
+
+def main():
+    asr_client = RivaASRClient()
+    nlu_client = RivaNLUClient()
+    tts_client = RivaTTSClient()
+    db_helper = SQLiteHelper()
+
+    # Transcribe speech to text
+    text = asr_client.transcribe("audio_samples/output_test.wav")
+    
+    #Get intent and slots from text
+    intent, slots = nlu_client.predict_intent_slots(text)
+    print(f"Intent: {intent}, Slots: {slots}")
+
+    answer = db_helper.query_database(intent, slots)
+    print(f"DB Query Result: {answer}")
+
+    # Convert text back to speech and save output WAV
+    tts_client.synthesize_and_save(text, "audio_samples/tts_output.wav")
+
+    db_helper.close()
+
+if __name__ == "__main__":
+    main()
