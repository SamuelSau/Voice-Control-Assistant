trainer:
  devices: 1
  num_nodes: 1
  max_epochs: 20
  max_steps: -1
  accumulate_grad_batches: 1
  precision: 16
  accelerator: gpu
  strategy: ddp
  log_every_n_steps: 1
  val_check_interval: 1.0
  enable_checkpointing: true
  logger: false
model:
  nemo_path: ./intent_slot_model.nemo
  data_dir: ./data
  class_labels:
    intent_labels_file: ./data/dict.intents.csv
    slot_labels_file: ./data/dict.slots.csv
  class_balancing: {}
  intent_loss_weight: 0.6
  pad_label: -1
  ignore_extra_tokens: false
  ignore_start_end: true

  train_ds:
    prefix: train
    batch_size: 16
    shuffle: true
    num_samples: -1
    num_workers: 4
    drop_last: false
    pin_memory: true

  validation_ds:
    prefix: val
    batch_size: 16
    shuffle: false
    num_samples: -1
    num_workers: 4
    drop_last: false
    pin_memory: true

  test_ds:
    prefix: test
    batch_size: 16
    shuffle: false
    num_samples: -1
    num_workers: 4
    drop_last: false
    pin_memory: true

  tokenizer:
    tokenizer_name: distilbert-base-uncased
    vocab_file: ../models/nlp/tokenizer.vocab_file
    tokenizer_model: null
    special_tokens: null

  language_model:
    max_seq_length: 64
    pretrained_model_name: distilbert-base-uncased
    lm_checkpoint: null
    config_file: ../models/nlp/distilbert-base-uncased_encoder_config.json
    config: null

  head:
    num_output_layers: 1
    fc_dropout: 0.1

  optim:
    name: adam
    lr: 2e-5
    args:
      name: auto
      params:
        weight_decay: 0.01

    sched:
      name: WarmupAnnealing
      iters_per_batch: null
      max_steps: -1
      monitor: val_loss
      reduce_on_plateau: false
      args:
        name: auto
        params:
          warmup_steps: null
          warmup_ratio: 0.1
          last_epoch: -1

exp_manager:
  exp_dir: ./nemo_experiments
  name: "IntentSlot_CustomDomain"
  create_tensorboard_logger: true
  create_checkpoint_callback: false

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null
