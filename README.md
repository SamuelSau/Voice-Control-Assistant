# Voice Control Assistant with RIVA

Project uses Nvidia RIVA with pretrained models for a full pipeline from ASR, NLP, and TTS that can query from SQLite with voice.

## Purpose for this project

I wanted to build an voice assistant to see if it could retrieve information from a database easily. I thought there was not many good solutions that provided a lot of the tooling that could be done out-of-the-box without doing a lot of configuration and experimentation. Essentially RIVA quick_start provides the ASR and TTS deployed models out-of-the-box, but did not have the NLP/NLU model that was useful for my case, so I decided to train my own.

## How this project works/runs

1. Add a .wav file within audio_samples/ (ensure that sample rate is 16,000)
2. Run ./riva_init.sh (if first time), and ./riva_start.sh with ./riva_start_client.sh
3. Run main.py from project root
4. A file named "tts_output.wav" will appear in audio_samples/ as the response to the entire pipeline (asr->nlp->tts)
5. (to modify data), apply changes within dict.intents.csv, dict.slots.csv, train.tsv, train_slots.tsv, etc...
6. (to modify database), apply changes to db_helper.py or populate_data.py (remember to remove assisstant.db since direct table modifications won't work)
7. **you might need riva-speech** to run those models, so grab container with `docker pull nvcr.io/nvidia/riva/riva-speech:2.19.0`
8. remember to have auth when you set the api key for `ngc config set` for downloading any containers/models from Nvidia

## Design choices

Since I had extracted the pretrained distilbert-uncased and the vocab lists from [SLU Conformer-Transformer-Large SLURP](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/slu_conformer_transformer_large_slurp), I believed that was enough as a baseline to train on. The configs yaml essentially contained the rest for the architecture and training of the model to generate the .nemo file.

## Data

All of the training, validation, and testing data were synthetically generated using a Python script based on the dict.intents.csv. About 80% of the slots .tsv files were token generated by ChatGPT but were then reverified manually to ensure that they aligned with the corresponding slot types. The data was split 80/10/10 for training, validation, and testing. Training was done in 50 epochs and ran on a singular GPU, my RTX 4070.

## Evaluation
| Test Metric       | DataLoader 0           |
|-------------------|------------------------|
| intent_f1         | 98.77%                 |
| intent_precision  | 98.77%                 |
| intent_recall     | 98.77%                 |
| slot_f1           | 94.87%                 |
| slot_precision    | 94.87%                 |
| slot_recall       | 94.87%                 |
| test_loss         | 0.208                  |

## Conclusion

Looking back, I might want to experiment by injecting more training examples and expanding the breadth for the project to limit test how well the model can achieve metrics and capabilties. If I had access to the Enterprise license, I definitely would use Nvidia's pretrained models to probably also train on as well. I realized that for intent and slot classfication, there is no conversion for the [nemo2riva](https://github.com/nvidia-riva/nemo2riva/tree/main#export-nemo-models-to-riva-with-nemo2riva) library that had a model that supported for my usecase. Therefore, I had to directly just reuse bits of the script I had before to run the .nemo on inference, as opposed to deploying with riva-build as a finalized .riva file for my pipeline.



### Links for General RIVA
- https://www.nvidia.com/en-us/ai-data-science/products/riva/
- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html
- https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart
- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html
- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/installation/deploy-local.html#local-docker
- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html
- https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix/support-matrix-older-versions.html#riva-2-17-0
- https://arizsiddiqui.medium.com/building-powerful-conversational-ai-models-with-nvidia-nemo-ce63243284d2
- https://github.com/nvidia-riva/tutorials?tab=readme-ov-file#running-the-riva-client
-https://docs.nvidia.com/deeplearning/riva/archives/170-b/user-guide/docs/notebooks/Riva_speech_API_demo.html
- https://github.com/nvidia-riva/python-clients
- https://docs.nvidia.com/nemo-framework/user-guide/24.07/nemotoolkit/asr/speech_intent_slot/configs.html

### Links for the NGC models used
- https://docs.nvidia.com/tao/tao-toolkit-archive/tao-30-2202/text/nlp/intent_and_slot.html


